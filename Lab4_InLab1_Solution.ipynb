{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO78RKtUHgale/QilQkcZB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KBVBrahmarao/datasciencecoursera/blob/main/Lab4_InLab1_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB4:\n",
        "In-Lab:\n",
        "1. Implement a MapReduce program to filter records based on a given condition.\n",
        "Example: Filter only the records where salary > 50,000, or lines containing a specific\n",
        "keyword (e.g., \"ERROR\").\n",
        " Write the Mapper logic for filtering.\n",
        " Use an Identity Reducer or write your own reducer.\n",
        " Run the program and capture output screenshots."
      ],
      "metadata": {
        "id": "IptarSFFoohS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 1 — Install Java\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-17-jdk-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yia5dlPyoxEm",
        "outputId": "852df9f8-69c1-4c11-9fe3-1be02884894d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-17-jdk-headless is already the newest version (17.0.17+10-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check Java:\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AmwW4Mip9Wd",
        "outputId": "d85bfa91-ec55-4a0e-890d-1e19cf227e9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"17.0.17\" 2025-10-21\n",
            "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 2 — Set Java Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-17-openjdk-amd64/bin\""
      ],
      "metadata": {
        "id": "kfUj3RhTp_9B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 3 — Download and Extract Hadoop\n",
        "!wget -q https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar -xzf hadoop-3.3.6.tar.gz\n",
        "!mv hadoop-3.3.6 /content/hadoop"
      ],
      "metadata": {
        "id": "EOHAqWtpqEuD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4 — Set Hadoop Environment Variables\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop\"\n",
        "os.environ[\"HADOOP_CONF_DIR\"] = \"/content/hadoop/etc/hadoop\"\n",
        "os.environ[\"PATH\"] += \":/content/hadoop/bin:/content/hadoop/sbin\""
      ],
      "metadata": {
        "id": "mfbgmmlXqUPn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extra STEP 1 — Set JAVA_HOME again\n",
        "!export HADOOP_HOME=/content/Hadoop\n",
        "!export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
        "!export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin"
      ],
      "metadata": {
        "id": "IanLe_XyqYvy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extra Step 2 — Verify directories\n",
        "!ls /content/Hadoop/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojgX75Jjqtmj",
        "outputId": "81a48868-1775-4f96-da95-97cc84a7e956"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/Hadoop/bin': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ExtraSTEP 3 — Check Hadoop version again\n",
        "!export HADOOP_HOME=/content/Hadoop && export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 && export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin && hadoop version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3Nr1lYCq8P9",
        "outputId": "2def2bc6-3377-4142-94e3-7358ea71745c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Cannot execute /content/Hadoop/libexec/hadoop-config.sh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 5 — Correct Hadoop Configuration Files - core-site.xml\n",
        "%%writefile /content/hadoop/etc/hadoop/core-site.xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>file:///</value>\n",
        "    </property>\n",
        "</configuration>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj80C6YUrBHJ",
        "outputId": "613e36f7-e183-4646-bc0f-9250b3ce3f73"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/hadoop/etc/hadoop/core-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6 - Create hdfs-site.xml (even though HDFS cannot run in Colab)\n",
        "#Step 6 - Create hdfs-site.xml (even though HDFS cannot run in Colab)\n",
        "%%writefile /content/hadoop/etc/hadoop/hdfs-site.xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "</configuration>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiz-0d0grDNN",
        "outputId": "aba4b338-4da2-45ff-a64f-643b13e0c43d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/hadoop/etc/hadoop/hdfs-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP  7A — Verify that files are correct\n",
        "!cat /content/hadoop/etc/hadoop/core-site.xml\n",
        "!cat /content/hadoop/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NluuzkL9rJiN",
        "outputId": "abc11211-6e85-427d-95ea-2bbdd3a57911"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<configuration>\n",
            "    <property>\n",
            "        <name>fs.defaultFS</name>\n",
            "        <value>file:///</value>\n",
            "    </property>\n",
            "</configuration>\n",
            "<?xml version=\"1.0\"?>\n",
            "<configuration>\n",
            "    <property>\n",
            "        <name>dfs.replication</name>\n",
            "        <value>1</value>\n",
            "    </property>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7B - Verify the files were created\n",
        "!ls /content/hadoop/etc/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21JosCVlrN4F",
        "outputId": "667ebc50-48b1-44c7-af9f-59a11cf1544b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  kms-log4j.properties\n",
            "configuration.xsl\t\t  kms-site.xml\n",
            "container-executor.cfg\t\t  log4j.properties\n",
            "core-site.xml\t\t\t  mapred-env.cmd\n",
            "hadoop-env.cmd\t\t\t  mapred-env.sh\n",
            "hadoop-env.sh\t\t\t  mapred-queues.xml.template\n",
            "hadoop-metrics2.properties\t  mapred-site.xml\n",
            "hadoop-policy.xml\t\t  shellprofile.d\n",
            "hadoop-user-functions.sh.example  ssl-client.xml.example\n",
            "hdfs-rbf-site.xml\t\t  ssl-server.xml.example\n",
            "hdfs-site.xml\t\t\t  user_ec_policies.xml.template\n",
            "httpfs-env.sh\t\t\t  workers\n",
            "httpfs-log4j.properties\t\t  yarn-env.cmd\n",
            "httpfs-site.xml\t\t\t  yarn-env.sh\n",
            "kms-acls.xml\t\t\t  yarnservice-log4j.properties\n",
            "kms-env.sh\t\t\t  yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 7C — Set correct config directory\n",
        "%%bash\n",
        "export HADOOP_HOME=/content/hadoop\n",
        "export HADOOP_CONF_DIR=/content/hadoop/etc/hadoop\n",
        "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
        "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
        "\n",
        "hadoop version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC8bxYN_rRvX",
        "outputId": "1a0199a6-8857-4324-bea5-0327e1dd894f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /content/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 7D — Delete old output\n",
        "!rm -rf filteroutput"
      ],
      "metadata": {
        "id": "FMmgZr3mrVqz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NEXT IMPORTANT STEPs\n",
        "#STEP 7 - Fix log4j warning (optional but recommended)\n",
        "%%bash\n",
        "cat > /content/hadoop/etc/hadoop/log4j.properties <<'EOF'\n",
        "log4j.rootLogger=INFO, console\n",
        "log4j.appender.console=org.apache.log4j.ConsoleAppender\n",
        "log4j.appender.console.target=System.err\n",
        "log4j.appender.console.layout=org.apache.log4j.PatternLayout\n",
        "log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p %c: %m%n\n",
        "EOF"
      ],
      "metadata": {
        "id": "whVD759srdJt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 8 — Run a Sample MapReduce Job (IMPORTANT TEST)\n",
        "#Create an input file:\n",
        "%%writefile employees.txt\n",
        "John,45000\n",
        "Mary,72000\n",
        "David,55000\n",
        "Kiran,38000\n",
        "Priya,90000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txj0CuyRrhdP",
        "outputId": "4d9bdb9c-1fce-43ba-ee93-305800aa6646"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing employees.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapper in Python (filters salary > 50000)\n",
        "%%writefile mapper_filter.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "THRESHOLD = 50000  # filtering condition\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    if not line:\n",
        "        continue\n",
        "\n",
        "    name, salary = line.split(\",\")\n",
        "    salary = int(salary)\n",
        "\n",
        "    if salary > THRESHOLD:\n",
        "        # Emit key-value pair\n",
        "        print(f\"{name}\\t{salary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUBnVhtYuM1-",
        "outputId": "1b361d1b-2ca1-4ea6-a52f-f4fe86072f10"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper_filter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducer in Python (pass-through)\n",
        "%%writefile reducer_filter.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRlkqeEauazN",
        "outputId": "23b77658-6db6-44d6-cfd1-0c03c7ccf1eb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reducer_filter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 8.1\n",
        "%%bash\n",
        "hdfs dfs -mkdir -p /filterinput\n",
        "hdfs dfs -put -f employees.txt /filterinput\n",
        "hdfs dfs -ls /filterinput"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfs-wwf4r4ld",
        "outputId": "768d4da3-e374-4561-cb96-e07047d98d8a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root         58 2025-12-31 09:27 /filterinput/employees.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run MapReduce Example (Line Length)\n",
        "#Hadoop already provides example JARs.\n",
        "%%bash\n",
        "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
        "export HADOOP_HOME=/content/hadoop\n",
        "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
        "\n",
        "hdfs dfs -rm -r /filteroutput\n",
        "\n",
        "hadoop jar \\\n",
        "$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar \\\n",
        "grep /filterinput /filteroutput '.*'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwFVS3d6skv1",
        "outputId": "6de930cc-ae95-4288-da88-a7129e6be9a2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted /filteroutput\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-12-31 09:27:18,595 INFO  org.apache.hadoop.conf.Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2025-12-31 09:27:19,932 INFO  org.apache.hadoop.metrics2.impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2025-12-31 09:27:20,102 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2025-12-31 09:27:20,102 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2025-12-31 09:27:20,358 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1\n",
            "2025-12-31 09:27:20,391 INFO  org.apache.hadoop.mapreduce.JobSubmitter: number of splits:1\n",
            "2025-12-31 09:27:20,646 INFO  org.apache.hadoop.mapreduce.JobSubmitter: Submitting tokens for job: job_local703189619_0001\n",
            "2025-12-31 09:27:20,647 INFO  org.apache.hadoop.mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2025-12-31 09:27:20,941 INFO  org.apache.hadoop.mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2025-12-31 09:27:20,942 INFO  org.apache.hadoop.mapreduce.Job: Running job: job_local703189619_0001\n",
            "2025-12-31 09:27:20,944 INFO  org.apache.hadoop.mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2025-12-31 09:27:20,956 INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2025-12-31 09:27:20,957 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-12-31 09:27:20,957 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-12-31 09:27:20,958 INFO  org.apache.hadoop.mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2025-12-31 09:27:21,017 INFO  org.apache.hadoop.mapred.LocalJobRunner: Waiting for map tasks\n",
            "2025-12-31 09:27:21,018 INFO  org.apache.hadoop.mapred.LocalJobRunner: Starting task: attempt_local703189619_0001_m_000000_0\n",
            "2025-12-31 09:27:21,053 INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2025-12-31 09:27:21,054 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-12-31 09:27:21,054 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-12-31 09:27:21,076 INFO  org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-12-31 09:27:21,082 INFO  org.apache.hadoop.mapred.MapTask: Processing split: file:/filterinput/employees.txt:0+58\n",
            "2025-12-31 09:27:21,459 INFO  org.apache.hadoop.mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2025-12-31 09:27:21,459 INFO  org.apache.hadoop.mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2025-12-31 09:27:21,459 INFO  org.apache.hadoop.mapred.MapTask: soft limit at 83886080\n",
            "2025-12-31 09:27:21,459 INFO  org.apache.hadoop.mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2025-12-31 09:27:21,459 INFO  org.apache.hadoop.mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2025-12-31 09:27:21,464 INFO  org.apache.hadoop.mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2025-12-31 09:27:21,475 INFO  org.apache.hadoop.mapred.LocalJobRunner: \n",
            "2025-12-31 09:27:21,476 INFO  org.apache.hadoop.mapred.MapTask: Starting flush of map output\n",
            "2025-12-31 09:27:21,476 INFO  org.apache.hadoop.mapred.MapTask: Spilling map output\n",
            "2025-12-31 09:27:21,476 INFO  org.apache.hadoop.mapred.MapTask: bufstart = 0; bufend = 143; bufvoid = 104857600\n",
            "2025-12-31 09:27:21,476 INFO  org.apache.hadoop.mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
            "2025-12-31 09:27:21,489 INFO  org.apache.hadoop.mapred.MapTask: Finished spill 0\n",
            "2025-12-31 09:27:21,500 INFO  org.apache.hadoop.mapred.Task: Task:attempt_local703189619_0001_m_000000_0 is done. And is in the process of committing\n",
            "2025-12-31 09:27:21,503 INFO  org.apache.hadoop.mapred.LocalJobRunner: map\n",
            "2025-12-31 09:27:21,503 INFO  org.apache.hadoop.mapred.Task: Task 'attempt_local703189619_0001_m_000000_0' done.\n",
            "2025-12-31 09:27:21,510 INFO  org.apache.hadoop.mapred.Task: Final Counters for attempt_local703189619_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=281572\n",
            "\t\tFILE: Number of bytes written=917552\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=5\n",
            "\t\tMap output records=10\n",
            "\t\tMap output bytes=143\n",
            "\t\tMap output materialized bytes=125\n",
            "\t\tInput split bytes=96\n",
            "\t\tCombine input records=10\n",
            "\t\tCombine output records=6\n",
            "\t\tSpilled Records=6\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=19\n",
            "\t\tTotal committed heap usage (bytes)=264241152\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=74\n",
            "2025-12-31 09:27:21,510 INFO  org.apache.hadoop.mapred.LocalJobRunner: Finishing task: attempt_local703189619_0001_m_000000_0\n",
            "2025-12-31 09:27:21,511 INFO  org.apache.hadoop.mapred.LocalJobRunner: map task executor complete.\n",
            "2025-12-31 09:27:21,514 INFO  org.apache.hadoop.mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2025-12-31 09:27:21,514 INFO  org.apache.hadoop.mapred.LocalJobRunner: Starting task: attempt_local703189619_0001_r_000000_0\n",
            "2025-12-31 09:27:21,522 INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2025-12-31 09:27:21,522 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-12-31 09:27:21,522 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-12-31 09:27:21,523 INFO  org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-12-31 09:27:21,525 INFO  org.apache.hadoop.mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6a45dcdf\n",
            "2025-12-31 09:27:21,527 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-12-31 09:27:21,543 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2025-12-31 09:27:21,548 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_local703189619_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2025-12-31 09:27:21,594 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local703189619_0001_m_000000_0 decomp: 121 len: 125 to MEMORY\n",
            "2025-12-31 09:27:21,598 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput: Read 121 bytes from map-output for attempt_local703189619_0001_m_000000_0\n",
            "2025-12-31 09:27:21,600 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 121, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->121\n",
            "2025-12-31 09:27:21,603 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2025-12-31 09:27:21,604 INFO  org.apache.hadoop.mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-12-31 09:27:21,604 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2025-12-31 09:27:21,611 INFO  org.apache.hadoop.mapred.Merger: Merging 1 sorted segments\n",
            "2025-12-31 09:27:21,612 INFO  org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 118 bytes\n",
            "2025-12-31 09:27:21,614 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merged 1 segments, 121 bytes to disk to satisfy reduce memory limit\n",
            "2025-12-31 09:27:21,615 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 1 files, 125 bytes from disk\n",
            "2025-12-31 09:27:21,615 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2025-12-31 09:27:21,615 INFO  org.apache.hadoop.mapred.Merger: Merging 1 sorted segments\n",
            "2025-12-31 09:27:21,620 INFO  org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 118 bytes\n",
            "2025-12-31 09:27:21,620 INFO  org.apache.hadoop.mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-12-31 09:27:21,634 INFO  org.apache.hadoop.conf.Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2025-12-31 09:27:21,637 INFO  org.apache.hadoop.mapred.Task: Task:attempt_local703189619_0001_r_000000_0 is done. And is in the process of committing\n",
            "2025-12-31 09:27:21,638 INFO  org.apache.hadoop.mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-12-31 09:27:21,639 INFO  org.apache.hadoop.mapred.Task: Task attempt_local703189619_0001_r_000000_0 is allowed to commit now\n",
            "2025-12-31 09:27:21,640 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_local703189619_0001_r_000000_0' to file:/content/grep-temp-1841411952\n",
            "2025-12-31 09:27:21,643 INFO  org.apache.hadoop.mapred.LocalJobRunner: reduce > reduce\n",
            "2025-12-31 09:27:21,643 INFO  org.apache.hadoop.mapred.Task: Task 'attempt_local703189619_0001_r_000000_0' done.\n",
            "2025-12-31 09:27:21,645 INFO  org.apache.hadoop.mapred.Task: Final Counters for attempt_local703189619_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=281854\n",
            "\t\tFILE: Number of bytes written=917930\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=6\n",
            "\t\tReduce shuffle bytes=125\n",
            "\t\tReduce input records=6\n",
            "\t\tReduce output records=6\n",
            "\t\tSpilled Records=6\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=264241152\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=253\n",
            "2025-12-31 09:27:21,645 INFO  org.apache.hadoop.mapred.LocalJobRunner: Finishing task: attempt_local703189619_0001_r_000000_0\n",
            "2025-12-31 09:27:21,645 INFO  org.apache.hadoop.mapred.LocalJobRunner: reduce task executor complete.\n",
            "2025-12-31 09:27:21,954 INFO  org.apache.hadoop.mapreduce.Job: Job job_local703189619_0001 running in uber mode : false\n",
            "2025-12-31 09:27:21,955 INFO  org.apache.hadoop.mapreduce.Job:  map 100% reduce 100%\n",
            "2025-12-31 09:27:21,956 INFO  org.apache.hadoop.mapreduce.Job: Job job_local703189619_0001 completed successfully\n",
            "2025-12-31 09:27:21,962 INFO  org.apache.hadoop.mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=563426\n",
            "\t\tFILE: Number of bytes written=1835482\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=5\n",
            "\t\tMap output records=10\n",
            "\t\tMap output bytes=143\n",
            "\t\tMap output materialized bytes=125\n",
            "\t\tInput split bytes=96\n",
            "\t\tCombine input records=10\n",
            "\t\tCombine output records=6\n",
            "\t\tReduce input groups=6\n",
            "\t\tReduce shuffle bytes=125\n",
            "\t\tReduce input records=6\n",
            "\t\tReduce output records=6\n",
            "\t\tSpilled Records=12\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=19\n",
            "\t\tTotal committed heap usage (bytes)=528482304\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=74\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=253\n",
            "2025-12-31 09:27:21,982 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-12-31 09:27:22,000 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1\n",
            "2025-12-31 09:27:22,005 INFO  org.apache.hadoop.mapreduce.JobSubmitter: number of splits:1\n",
            "2025-12-31 09:27:22,028 INFO  org.apache.hadoop.mapreduce.JobSubmitter: Submitting tokens for job: job_local912839539_0002\n",
            "2025-12-31 09:27:22,028 INFO  org.apache.hadoop.mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2025-12-31 09:27:22,127 INFO  org.apache.hadoop.mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2025-12-31 09:27:22,127 INFO  org.apache.hadoop.mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2025-12-31 09:27:22,127 INFO  org.apache.hadoop.mapreduce.Job: Running job: job_local912839539_0002\n",
            "2025-12-31 09:27:22,128 INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2025-12-31 09:27:22,128 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-12-31 09:27:22,128 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-12-31 09:27:22,128 INFO  org.apache.hadoop.mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2025-12-31 09:27:22,132 INFO  org.apache.hadoop.mapred.LocalJobRunner: Waiting for map tasks\n",
            "2025-12-31 09:27:22,132 INFO  org.apache.hadoop.mapred.LocalJobRunner: Starting task: attempt_local912839539_0002_m_000000_0\n",
            "2025-12-31 09:27:22,134 INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2025-12-31 09:27:22,134 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-12-31 09:27:22,134 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-12-31 09:27:22,134 INFO  org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-12-31 09:27:22,136 INFO  org.apache.hadoop.mapred.MapTask: Processing split: file:/content/grep-temp-1841411952/part-r-00000:0+241\n",
            "2025-12-31 09:27:22,157 INFO  org.apache.hadoop.mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2025-12-31 09:27:22,157 INFO  org.apache.hadoop.mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2025-12-31 09:27:22,158 INFO  org.apache.hadoop.mapred.MapTask: soft limit at 83886080\n",
            "2025-12-31 09:27:22,158 INFO  org.apache.hadoop.mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2025-12-31 09:27:22,158 INFO  org.apache.hadoop.mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2025-12-31 09:27:22,159 INFO  org.apache.hadoop.mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2025-12-31 09:27:22,167 INFO  org.apache.hadoop.mapred.LocalJobRunner: \n",
            "2025-12-31 09:27:22,168 INFO  org.apache.hadoop.mapred.MapTask: Starting flush of map output\n",
            "2025-12-31 09:27:22,168 INFO  org.apache.hadoop.mapred.MapTask: Spilling map output\n",
            "2025-12-31 09:27:22,168 INFO  org.apache.hadoop.mapred.MapTask: bufstart = 0; bufend = 107; bufvoid = 104857600\n",
            "2025-12-31 09:27:22,168 INFO  org.apache.hadoop.mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n",
            "2025-12-31 09:27:22,170 INFO  org.apache.hadoop.mapred.MapTask: Finished spill 0\n",
            "2025-12-31 09:27:22,173 INFO  org.apache.hadoop.mapred.Task: Task:attempt_local912839539_0002_m_000000_0 is done. And is in the process of committing\n",
            "2025-12-31 09:27:22,174 INFO  org.apache.hadoop.mapred.LocalJobRunner: map\n",
            "2025-12-31 09:27:22,174 INFO  org.apache.hadoop.mapred.Task: Task 'attempt_local912839539_0002_m_000000_0' done.\n",
            "2025-12-31 09:27:22,175 INFO  org.apache.hadoop.mapred.Task: Final Counters for attempt_local912839539_0002_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=563622\n",
            "\t\tFILE: Number of bytes written=1834192\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6\n",
            "\t\tMap output records=6\n",
            "\t\tMap output bytes=107\n",
            "\t\tMap output materialized bytes=125\n",
            "\t\tInput split bytes=112\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=6\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tTotal committed heap usage (bytes)=264241152\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=253\n",
            "2025-12-31 09:27:22,175 INFO  org.apache.hadoop.mapred.LocalJobRunner: Finishing task: attempt_local912839539_0002_m_000000_0\n",
            "2025-12-31 09:27:22,175 INFO  org.apache.hadoop.mapred.LocalJobRunner: map task executor complete.\n",
            "2025-12-31 09:27:22,176 INFO  org.apache.hadoop.mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2025-12-31 09:27:22,176 INFO  org.apache.hadoop.mapred.LocalJobRunner: Starting task: attempt_local912839539_0002_r_000000_0\n",
            "2025-12-31 09:27:22,178 INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2025-12-31 09:27:22,178 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-12-31 09:27:22,178 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-12-31 09:27:22,178 INFO  org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-12-31 09:27:22,178 INFO  org.apache.hadoop.mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3011a4db\n",
            "2025-12-31 09:27:22,179 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-12-31 09:27:22,180 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2025-12-31 09:27:22,185 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local912839539_0002_m_000000_0 decomp: 121 len: 125 to MEMORY\n",
            "2025-12-31 09:27:22,185 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_local912839539_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2025-12-31 09:27:22,185 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput: Read 121 bytes from map-output for attempt_local912839539_0002_m_000000_0\n",
            "2025-12-31 09:27:22,186 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 121, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->121\n",
            "2025-12-31 09:27:22,186 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2025-12-31 09:27:22,188 INFO  org.apache.hadoop.mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-12-31 09:27:22,189 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2025-12-31 09:27:22,190 INFO  org.apache.hadoop.mapred.Merger: Merging 1 sorted segments\n",
            "2025-12-31 09:27:22,190 INFO  org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 111 bytes\n",
            "2025-12-31 09:27:22,191 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merged 1 segments, 121 bytes to disk to satisfy reduce memory limit\n",
            "2025-12-31 09:27:22,191 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 1 files, 125 bytes from disk\n",
            "2025-12-31 09:27:22,191 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2025-12-31 09:27:22,191 INFO  org.apache.hadoop.mapred.Merger: Merging 1 sorted segments\n",
            "2025-12-31 09:27:22,192 INFO  org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 111 bytes\n",
            "2025-12-31 09:27:22,192 INFO  org.apache.hadoop.mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-12-31 09:27:22,198 INFO  org.apache.hadoop.mapred.Task: Task:attempt_local912839539_0002_r_000000_0 is done. And is in the process of committing\n",
            "2025-12-31 09:27:22,199 INFO  org.apache.hadoop.mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-12-31 09:27:22,199 INFO  org.apache.hadoop.mapred.Task: Task attempt_local912839539_0002_r_000000_0 is allowed to commit now\n",
            "2025-12-31 09:27:22,201 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_local912839539_0002_r_000000_0' to file:/filteroutput\n",
            "2025-12-31 09:27:22,202 INFO  org.apache.hadoop.mapred.LocalJobRunner: reduce > reduce\n",
            "2025-12-31 09:27:22,203 INFO  org.apache.hadoop.mapred.Task: Task 'attempt_local912839539_0002_r_000000_0' done.\n",
            "2025-12-31 09:27:22,203 INFO  org.apache.hadoop.mapred.Task: Final Counters for attempt_local912839539_0002_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=563904\n",
            "\t\tFILE: Number of bytes written=1834400\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=2\n",
            "\t\tReduce shuffle bytes=125\n",
            "\t\tReduce input records=6\n",
            "\t\tReduce output records=6\n",
            "\t\tSpilled Records=6\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=264241152\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=83\n",
            "2025-12-31 09:27:22,203 INFO  org.apache.hadoop.mapred.LocalJobRunner: Finishing task: attempt_local912839539_0002_r_000000_0\n",
            "2025-12-31 09:27:22,203 INFO  org.apache.hadoop.mapred.LocalJobRunner: reduce task executor complete.\n",
            "2025-12-31 09:27:23,128 INFO  org.apache.hadoop.mapreduce.Job: Job job_local912839539_0002 running in uber mode : false\n",
            "2025-12-31 09:27:23,129 INFO  org.apache.hadoop.mapreduce.Job:  map 100% reduce 100%\n",
            "2025-12-31 09:27:23,129 INFO  org.apache.hadoop.mapreduce.Job: Job job_local912839539_0002 completed successfully\n",
            "2025-12-31 09:27:23,132 INFO  org.apache.hadoop.mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1127526\n",
            "\t\tFILE: Number of bytes written=3668592\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6\n",
            "\t\tMap output records=6\n",
            "\t\tMap output bytes=107\n",
            "\t\tMap output materialized bytes=125\n",
            "\t\tInput split bytes=112\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=2\n",
            "\t\tReduce shuffle bytes=125\n",
            "\t\tReduce input records=6\n",
            "\t\tReduce output records=6\n",
            "\t\tSpilled Records=12\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tTotal committed heap usage (bytes)=528482304\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=253\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#View Output\n",
        "%%bash\n",
        "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
        "export HADOOP_HOME=/content/hadoop\n",
        "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
        "\n",
        "hdfs dfs -cat /filteroutput/part-r-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f87V-2gctWZN",
        "outputId": "313a5738-6b71-4ba0-89c8-e69fb7a89efd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\t\n",
            "1\tPriya,90000\n",
            "1\tMary,72000\n",
            "1\tKiran,38000\n",
            "1\tJohn,45000\n",
            "1\tDavid,55000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat employees.txt | python3 mapper_filter.py | python3 reducer_filter.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5QBITDtvGFU",
        "outputId": "5e26a0c5-58fb-403d-dff8-75f4f095fee7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary\t72000\n",
            "David\t55000\n",
            "Priya\t90000\n"
          ]
        }
      ]
    }
  ]
}